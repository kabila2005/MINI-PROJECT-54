{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5547005,"sourceType":"datasetVersion","datasetId":3196060}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis \n- Private dataset with sensitive hospital comments ","metadata":{}},{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport pandas as pd\nimport spacy\nimport en_core_web_sm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:33.524757Z","iopub.execute_input":"2023-11-07T09:55:33.525114Z","iopub.status.idle":"2023-11-07T09:55:48.867179Z","shell.execute_reply.started":"2023-11-07T09:55:33.525085Z","shell.execute_reply":"2023-11-07T09:55:48.866028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('wordnet')\nnltk.download('wordnet2022')\n\n! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:48.869262Z","iopub.execute_input":"2023-11-07T09:55:48.869959Z","iopub.status.idle":"2023-11-07T09:55:50.067251Z","shell.execute_reply.started":"2023-11-07T09:55:48.869928Z","shell.execute_reply":"2023-11-07T09:55:50.065384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/private-hospital-comments/comments1.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:50.069144Z","iopub.execute_input":"2023-11-07T09:55:50.069503Z","iopub.status.idle":"2023-11-07T09:55:50.151756Z","shell.execute_reply.started":"2023-11-07T09:55:50.069457Z","shell.execute_reply":"2023-11-07T09:55:50.150316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing the Dataset ","metadata":{}},{"cell_type":"code","source":"df[\"Main Topic\"] = df[\"Main Topic\"].fillna(\"No Topic Given\")\ndf = df.dropna(subset=[\"Content\"])\nnan_counts = df.isna().sum()\n\nprint(nan_counts)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:50.154825Z","iopub.execute_input":"2023-11-07T09:55:50.155157Z","iopub.status.idle":"2023-11-07T09:55:50.190464Z","shell.execute_reply.started":"2023-11-07T09:55:50.155122Z","shell.execute_reply":"2023-11-07T09:55:50.189476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining regex patterns.\nlinebreaks        = \"<br /><br />\"\nalphaPattern      = \"[^a-z0-9<>]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n# Defining regex for emojis\nsmileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\nsademoji          = r\"[8:=;]['`\\-]?\\(+\"\nneutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\nlolemoji          = r\"[8:=;]['`\\-]?p+\"\n\nstop_words = set(stopwords.words('english'))\nLemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:50.192212Z","iopub.execute_input":"2023-11-07T09:55:50.192682Z","iopub.status.idle":"2023-11-07T09:55:50.202427Z","shell.execute_reply.started":"2023-11-07T09:55:50.192634Z","shell.execute_reply":"2023-11-07T09:55:50.200425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_reviews(review):\n\n    review = review.lower()\n\n    review = re.sub(linebreaks,\" \",review)\n    # Replace 3 or more consecutive letters by 2 letter.\n    review = re.sub(sequencePattern, seqReplacePattern, review)\n\n    # Replace all emojis.\n    review = re.sub(r'<3', '<heart>', review)\n    review = re.sub(smileemoji, '<smile>', review)\n    review = re.sub(sademoji, '<sadface>', review)\n    review = re.sub(neutralemoji, '<neutralface>', review)\n    review = re.sub(lolemoji, '<lolface>', review)\n\n    # Remove non-alphanumeric and symbols\n    review = re.sub(alphaPattern, ' ', review)\n    \n    # Tokenize the input text\n    tokens = word_tokenize(review)\n    \n    # Remove stop words from the token sequence\n\n    tokens = [token for token in tokens if token not in stop_words]\n    \n    # Lemmatize the remaining tokens\n    tokens = [Lemmatizer.lemmatize(token) for token in tokens]\n    \n    # Join the cleaned tokens into a single string\n    return ' '.join(tokens)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:50.204333Z","iopub.execute_input":"2023-11-07T09:55:50.205333Z","iopub.status.idle":"2023-11-07T09:55:50.215508Z","shell.execute_reply.started":"2023-11-07T09:55:50.205288Z","shell.execute_reply":"2023-11-07T09:55:50.213895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine \"Main Topic\", \"Subtopic\", and \"Content\" columns into a single column called \"Text\"\ndf[\"Text\"] = df[\"Main Topic\"] + \" \" + df[\"Subtopic\"] + \" \" + df[\"Content\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:50.21697Z","iopub.execute_input":"2023-11-07T09:55:50.217296Z","iopub.status.idle":"2023-11-07T09:55:50.240056Z","shell.execute_reply.started":"2023-11-07T09:55:50.217271Z","shell.execute_reply":"2023-11-07T09:55:50.238887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Text\"] = df[\"Text\"].apply(preprocess_reviews)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:50.241715Z","iopub.execute_input":"2023-11-07T09:55:50.242093Z","iopub.status.idle":"2023-11-07T09:55:54.204627Z","shell.execute_reply.started":"2023-11-07T09:55:50.242061Z","shell.execute_reply":"2023-11-07T09:55:54.20287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keyword and Sentiment Analysis using Word2Vec \n- Reference Article: https://towardsdatascience.com/unsupervised-semantic-sentiment-analysis-of-imdb-reviews-2c5f520fbf81","metadata":{}},{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/towardsNLP/IMDB-Semantic-Sentiment-Analysis/main/Word2Vec/src/w2v_utils.py -o w2v_utils.py","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:54.206233Z","iopub.execute_input":"2023-11-07T09:55:54.206613Z","iopub.status.idle":"2023-11-07T09:55:54.663724Z","shell.execute_reply.started":"2023-11-07T09:55:54.206579Z","shell.execute_reply":"2023-11-07T09:55:54.662839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from w2v_utils import (Tokenizer,\n                       evaluate_model,\n                       bow_vectorizer,\n                       train_logistic_regressor,\n                       w2v_trainer,\n                       calculate_overall_similarity_score,\n                       overall_semantic_sentiment_analysis,\n                       list_similarity,\n                       calculate_topn_similarity_score,\n                       topn_semantic_sentiment_analysis,\n                       define_complexity_subjectivity_reviews,\n                       explore_high_complexity_reviews,\n                       explore_low_subjectivity_reviews,\n                       text_SSA)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:54.668957Z","iopub.execute_input":"2023-11-07T09:55:54.669325Z","iopub.status.idle":"2023-11-07T09:55:55.849624Z","shell.execute_reply.started":"2023-11-07T09:55:54.669296Z","shell.execute_reply":"2023-11-07T09:55:55.848317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Instancing the Tokenizer class\ntokenizer = Tokenizer(clean= True,\n                      lower= True, \n                      de_noise= True, \n                      remove_stop_words= True,\n                      keep_negation=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:55.851215Z","iopub.execute_input":"2023-11-07T09:55:55.851602Z","iopub.status.idle":"2023-11-07T09:55:55.857396Z","shell.execute_reply.started":"2023-11-07T09:55:55.85157Z","shell.execute_reply":"2023-11-07T09:55:55.856281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tokenized_text'] = df['Text'].apply(tokenizer.tokenize)\n\ndf['tokenized_text_len'] = df['tokenized_text'].apply(len)\ndf['tokenized_text_len'].apply(np.log).describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:55.858669Z","iopub.execute_input":"2023-11-07T09:55:55.858966Z","iopub.status.idle":"2023-11-07T09:55:56.454563Z","shell.execute_reply.started":"2023-11-07T09:55:55.858939Z","shell.execute_reply":"2023-11-07T09:55:56.452946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors , keyed_vocab = w2v_trainer(df[\"tokenized_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:56.455988Z","iopub.execute_input":"2023-11-07T09:55:56.456264Z","iopub.status.idle":"2023-11-07T09:55:58.235402Z","shell.execute_reply.started":"2023-11-07T09:55:56.45624Z","shell.execute_reply":"2023-11-07T09:55:58.234141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(keyed_vectors))\nprint(type(keyed_vocab))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.236544Z","iopub.execute_input":"2023-11-07T09:55:58.236768Z","iopub.status.idle":"2023-11-07T09:55:58.242238Z","shell.execute_reply.started":"2023-11-07T09:55:58.236746Z","shell.execute_reply":"2023-11-07T09:55:58.241279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors.most_similar(\"research\",topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.243399Z","iopub.execute_input":"2023-11-07T09:55:58.243638Z","iopub.status.idle":"2023-11-07T09:55:58.259387Z","shell.execute_reply.started":"2023-11-07T09:55:58.243615Z","shell.execute_reply":"2023-11-07T09:55:58.258489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors.most_similar(\"hospital\",topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.260863Z","iopub.execute_input":"2023-11-07T09:55:58.261388Z","iopub.status.idle":"2023-11-07T09:55:58.269902Z","shell.execute_reply.started":"2023-11-07T09:55:58.261356Z","shell.execute_reply":"2023-11-07T09:55:58.268904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyed_vectors.most_similar(\"funded\",topn=15)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.271485Z","iopub.execute_input":"2023-11-07T09:55:58.272085Z","iopub.status.idle":"2023-11-07T09:55:58.27922Z","shell.execute_reply.started":"2023-11-07T09:55:58.272057Z","shell.execute_reply":"2023-11-07T09:55:58.27842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering Approach to Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"# To make sure that all `positive_concepts` are in the keyed word2vec vocabulary\npositive_concepts = ['excellent', 'awesome', 'cool','decent','amazing', 'strong', 'good', 'great', 'funny', 'entertaining'] \npos_concepts = [concept for concept in positive_concepts if concept in keyed_vocab]","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.280718Z","iopub.execute_input":"2023-11-07T09:55:58.281355Z","iopub.status.idle":"2023-11-07T09:55:58.285791Z","shell.execute_reply.started":"2023-11-07T09:55:58.281327Z","shell.execute_reply":"2023-11-07T09:55:58.284995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To make sure that all `negative_concepts` are in the keyed word2vec vocabulary \nnegative_concepts = ['terrible','awful','horrible','boring','bad', 'disappointing', 'weak', 'poor',  'senseless','confusing'] \nneg_concepts = [concept for concept in negative_concepts if concept in keyed_vocab]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.287125Z","iopub.execute_input":"2023-11-07T09:55:58.287694Z","iopub.status.idle":"2023-11-07T09:55:58.295869Z","shell.execute_reply.started":"2023-11-07T09:55:58.287668Z","shell.execute_reply":"2023-11-07T09:55:58.295098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating Semantic Sentiment Scores by OSSA model\noverall_df_scores = overall_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n                                                   positive_target_tokens = pos_concepts, \n                                                   negative_target_tokens = neg_concepts,\n                                                   doc_tokens = df['tokenized_text'])\n\n# Calculating Semantic Sentiment Scores by TopSSA model\ntopn_df_scores = topn_semantic_sentiment_analysis (keyed_vectors = keyed_vectors,\n                                                   positive_target_tokens = pos_concepts, \n                                                   negative_target_tokens = neg_concepts,\n                                                   doc_tokens = df['tokenized_text'],\n                                                     topn=30)\n\n\n# To store semantic sentiment store computed by OSSA model in df\ndf['overall_PSS'] = overall_df_scores[0] \ndf['overall_NSS'] = overall_df_scores[1] \ndf['overall_semantic_sentiment_score'] = overall_df_scores[2] \ndf['overall_semantic_sentiment_polarity'] = overall_df_scores[3]\n\n\n\n# To store semantic sentiment store computed by TopSSA model in df\ndf['topn_PSS'] = topn_df_scores[0] \ndf['topn_NSS'] = topn_df_scores[1] \ndf['topn_semantic_sentiment_score'] = topn_df_scores[2] \ndf['topn_semantic_sentiment_polarity'] = topn_df_scores[3]","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:55:58.297252Z","iopub.execute_input":"2023-11-07T09:55:58.297879Z","iopub.status.idle":"2023-11-07T09:56:02.579556Z","shell.execute_reply.started":"2023-11-07T09:55:58.297817Z","shell.execute_reply":"2023-11-07T09:56:02.57825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = keyed_vectors.index_to_key\nvectors = [keyed_vectors[word] for word in words]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:02.580676Z","iopub.execute_input":"2023-11-07T09:56:02.580992Z","iopub.status.idle":"2023-11-07T09:56:02.593838Z","shell.execute_reply.started":"2023-11-07T09:56:02.580967Z","shell.execute_reply":"2023-11-07T09:56:02.592746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nresult = pca.fit_transform(vectors)\n\n# Create a DataFrame with PCA results and words\npca_df = pd.DataFrame(result, columns=['x', 'y'])\npca_df['word'] = words\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:02.596677Z","iopub.execute_input":"2023-11-07T09:56:02.59702Z","iopub.status.idle":"2023-11-07T09:56:02.720379Z","shell.execute_reply.started":"2023-11-07T09:56:02.596993Z","shell.execute_reply":"2023-11-07T09:56:02.719671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\n\nfig = go.Figure(data=go.Scattergl(\n    x=pca_df['x'],\n    y=pca_df['y'],\n    mode='markers',\n    marker=dict(\n        colorscale='Viridis',\n        line_width=1\n    ),\n    text=pca_df['word'],\n    textposition=\"bottom center\"\n))\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:02.721619Z","iopub.execute_input":"2023-11-07T09:56:02.722034Z","iopub.status.idle":"2023-11-07T09:56:02.967894Z","shell.execute_reply.started":"2023-11-07T09:56:02.722007Z","shell.execute_reply":"2023-11-07T09:56:02.965725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_pos_filt = df['topn_semantic_sentiment_polarity'] == 1\nactual_neg_filt =  df['topn_semantic_sentiment_polarity'] == 0\n\n# filter positive and negative review based on Most Probable predicted 'y' or 'topn_semantic_sentiment_score' column\npredicted_pos_filt = df['topn_semantic_sentiment_polarity'] == 1\npredicted_neg_filt = df['topn_semantic_sentiment_polarity'] == 0\n\n\n\n# plotting Semantic Sentiment Score Position of Actual Negative Reviews \nplt.scatter(df['topn_NSS'][actual_neg_filt], \n         df['topn_PSS'][actual_neg_filt],  \n         label='Actual Negetive Reviews',\n           color='DarkRed',\n            alpha=0.4 , # set transparency of color\n            s=20 # set size of dots\n           )\n\n# plotting Semantic Sentiment Score Position of Actual Positive Reviews \nplt.scatter(df['topn_NSS'][actual_pos_filt], \n         df['topn_PSS'][actual_pos_filt],  \n         label='Actual Positive Reviews',\n       color='DarkGreen',\n            alpha=0.1, # set transparency of color\n            s=30 # set size of dots\n           )\n# naming the x & y axis\nplt.xlabel('Predicted Negative Labels')\nplt.ylabel('Predicted Positive Labels')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:02.970145Z","iopub.execute_input":"2023-11-07T09:56:02.973198Z","iopub.status.idle":"2023-11-07T09:56:03.633102Z","shell.execute_reply.started":"2023-11-07T09:56:02.973143Z","shell.execute_reply":"2023-11-07T09:56:03.631408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis Using BERT","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:03.634405Z","iopub.execute_input":"2023-11-07T09:56:03.634685Z","iopub.status.idle":"2023-11-07T09:56:14.597826Z","shell.execute_reply.started":"2023-11-07T09:56:03.634662Z","shell.execute_reply":"2023-11-07T09:56:14.596944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# importing the pipeline module\nfrom transformers import pipeline\n \n# Downloading the sentiment analysis model\nSentimentClassifier = pipeline(\"sentiment-analysis\")\n\n# Downloading the sentiment analysis model\n# SentimentClassifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:14.598777Z","iopub.execute_input":"2023-11-07T09:56:14.599108Z","iopub.status.idle":"2023-11-07T09:56:22.78749Z","shell.execute_reply.started":"2023-11-07T09:56:14.599081Z","shell.execute_reply":"2023-11-07T09:56:22.78598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to call for the whole dataframe\ndef FunctionBERTSentiment(inpText):\n  return(SentimentClassifier(inpText)[0]['label'])","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:22.788826Z","iopub.execute_input":"2023-11-07T09:56:22.789141Z","iopub.status.idle":"2023-11-07T09:56:22.794063Z","shell.execute_reply.started":"2023-11-07T09:56:22.789117Z","shell.execute_reply":"2023-11-07T09:56:22.793107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['BERT_Sentiment']=df['Text'].apply(FunctionBERTSentiment)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T09:56:22.799296Z","iopub.execute_input":"2023-11-07T09:56:22.799617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to call for the whole dataframe\ndef FunctionBERTScore(inpText):\n  return(SentimentClassifier(inpText)[0]['score'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Score']=df['Text'].apply(FunctionBERTScore)\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Content_BERT_Sentiment']=df['Content'].apply(FunctionBERTSentiment)\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('bert_sentiment.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, subPlot =plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nfig.suptitle(\"Sentiment analysis of Content + Topic Text\")\n \n# Grouping the data\nGroupedData=df.groupby('BERT_Sentiment').size()\n \n# Creating the charts\nGroupedData.plot(kind='bar', ax=subPlot[0], color=['crimson', 'lightblue'])\nGroupedData.plot(kind='pie', ax=subPlot[1], colors=['crimson', 'lightblue'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, subPlot =plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nfig.suptitle(\"Sentiment analysis of Content Only\")\n \n# Grouping the data\nGroupedData=df.groupby('Content_BERT_Sentiment').size()\n \n# Creating the charts\nGroupedData.plot(kind='bar', ax=subPlot[0], color=['crimson', 'lightblue'])\nGroupedData.plot(kind='pie', ax=subPlot[1], colors=['crimson', 'lightblue'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}